{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4 , stride = stride, bias =False , padding_mode= 'reflect'),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=2, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, features[0], kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        inchannels = features[0]\n",
    "        for feature in features:\n",
    "            layers.append(\n",
    "                CNNBlock(inchannels, feature, stride=1 if feature == features[-1] else 2),\n",
    "            )\n",
    "            inchannels = feature  # Update to the current feature size\n",
    "\n",
    "        # Move the last convolution outside of the loop\n",
    "        layers.append(\n",
    "            nn.Conv2d(inchannels, 1, kernel_size=4, stride=1, padding=1, padding_mode='reflect')\n",
    "        )\n",
    "\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z = torch.cat([x, y], dim=1)  # Ensure x and y are both of shape [1, 1, 62, 62]\n",
    "        return self.layer(self.initial(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test case for patch discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 256, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((1, 3, 256, 256))  # First input (e.g., real image)\n",
    "y = torch.randn((1, 3, 256, 256)) \n",
    "z=torch.cat([x,y],dim=1)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 1, 10, 10])\n",
      "Predictions: tensor([[[[ 0.3128, -0.0162, -0.2867,  0.1372, -0.0033,  0.1759,  0.5144,\n",
      "            0.2245,  0.5993,  0.9960],\n",
      "          [ 0.5208,  0.5942,  0.2155, -0.1409,  0.5804,  0.0968,  0.5500,\n",
      "            0.3194, -0.1017,  0.1733],\n",
      "          [ 0.3673,  0.3999,  0.4392,  0.8167, -0.0501,  0.4871,  0.1982,\n",
      "            0.6552, -0.5762,  0.5614],\n",
      "          [ 0.3511,  0.5382, -0.0458,  0.6049, -0.1525,  0.1308,  0.2330,\n",
      "            0.4913,  1.1680,  0.3298],\n",
      "          [-0.3581,  0.3010,  0.5054,  0.4582,  0.8617,  0.3152, -0.3276,\n",
      "            0.1838,  0.3992,  0.0107],\n",
      "          [-0.2541,  0.5760, -0.1197,  0.1642, -0.1338,  0.9193,  0.3654,\n",
      "            0.5435,  0.2882,  0.2399],\n",
      "          [ 0.2375,  0.2074,  0.0512,  0.1200,  0.3323,  0.5569,  0.1875,\n",
      "            0.1244,  0.2603,  0.6403],\n",
      "          [ 0.6800,  0.0412, -0.6289,  0.7676,  0.2540, -0.0647,  0.7626,\n",
      "           -0.3743,  0.9909, -0.1344],\n",
      "          [ 0.2194,  0.6420,  0.7806,  0.3093,  0.1291,  0.3294,  0.5460,\n",
      "            0.1808,  0.2948,  0.2525],\n",
      "          [ 0.1595, -0.0658, -0.1940,  0.4970, -0.3361,  0.1114,  0.2609,\n",
      "            0.5326, -0.0170,  0.3122]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def test_discriminator():\n",
    "    \n",
    "    x = torch.randn((1, 2, 256, 256)) \n",
    "    y = torch.randn((1, 2, 256, 256))  \n",
    "    \n",
    "    test_model_discriminator = Discriminator()\n",
    "    preds = test_model_discriminator(x, y)\n",
    "    print(\"Output shape:\", preds.shape)\n",
    "    print(\"Predictions:\", preds)\n",
    "\n",
    "\n",
    "test_discriminator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Genarator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act='relu',use_dropout=False):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4 , stride = 2, padding=1,bias =False , padding_mode= 'reflect')\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4 , stride = 2, padding=1,bias =False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
    "            )\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x= self.conv(x)\n",
    "        return self.dropout(x)if self.use_dropout else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels, features =64):\n",
    "        super().__init__()\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,features,4,2,1,padding_mode='reflect'),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.down1 = Block(features, features*2, down=True, act = \"leakyrelu\", use_dropout=\"false\")\n",
    "        self.down2 = Block(features*2, features*4, down=True, act = \"leakyrelu\", use_dropout=\"false\")\n",
    "        self.down3 = Block(features*4, features*8, down=True, act = \"leakyrelu\", use_dropout=\"false\")\n",
    "        self.down4 = Block(features*8, features*8, down=True, act = \"leakyrelu\", use_dropout=\"false\")\n",
    "        self.down5 = Block(features*8, features*8, down=True, act = \"leakyrelu\", use_dropout=\"false\")\n",
    "        self.down6 = Block(features*8, features*8, down=True, act = \"leakyrelu\", use_dropout=\"false\")\n",
    "        self.bottom_layer = nn.Sequential(\n",
    "            nn.Conv2d(features*8,features*8,4,2,1,padding_mode=\"reflect\"),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up1 = Block(features*8, features*8, down=False, act = \"relu\", use_dropout=\"True\")\n",
    "        self.up2 = Block(features*8*2, features*8, down=False, act = \"relu\", use_dropout=\"True\")\n",
    "        self.up3 = Block(features*8*2, features*8, down=False, act = \"relu\", use_dropout=\"True\")\n",
    "        self.up4 = Block(features*8*2, features*8, down=False, act = \"relu\", use_dropout=\"false\")\n",
    "        self.up5 = Block(features*8*2, features*4, down=False, act = \"relu\", use_dropout=\"false\")\n",
    "        self.up6 = Block(features*4*2, features*2, down=False, act = \"relu\", use_dropout=\"false\")\n",
    "        self.up7 = Block(features*2*2, features, down=False ,act = \"relu\", use_dropout=\"false\")\n",
    "        self.top_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2,3, kernel_size=4, stride =2 , padding =1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.initial_down(x)  \n",
    "        d2 = self.down1(d1)        \n",
    "        d3 = self.down2(d2)        \n",
    "        d4 = self.down3(d3)        \n",
    "        d5 = self.down4(d4)        \n",
    "        d6 = self.down5(d5)    \n",
    "        d7 = self.down6(d6)    \n",
    "        last_down = self.bottom_layer(d7)  \n",
    "\n",
    "        up1 = self.up1(last_down)  \n",
    "        up2 = self.up2(torch.cat([up1, d7], 1))  \n",
    "        up3 = self.up3(torch.cat([up2, d6], 1))  \n",
    "        up4 = self.up4(torch.cat([up3, d5], 1))  \n",
    "        up5 = self.up5(torch.cat([up4, d4], 1))  \n",
    "        up6 = self.up6(torch.cat([up5, d3], 1))  \n",
    "        up7 = self.up7(torch.cat([up6, d2], 1))  \n",
    "    \n",
    "        return self.top_layer(torch.cat([up7, d1], 1))  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator():\n",
    "    x = torch.randn((1,1,256,256))\n",
    "    model = UnetGenerator(in_channels=1, features=64)\n",
    "    preds = model(x)\n",
    "    print(preds.shape)\n",
    "    print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "tensor([[[[-0.3342,  0.4635, -0.4298,  ...,  0.1046,  0.2946, -0.2066],\n",
      "          [ 0.1404, -0.6600, -0.0810,  ...,  0.9957,  0.8506,  0.3114],\n",
      "          [ 0.0954,  0.8522, -0.5192,  ...,  0.4843,  0.0160, -0.5520],\n",
      "          ...,\n",
      "          [-0.3403,  0.6162,  0.0026,  ...,  0.4549,  0.8869, -0.2023],\n",
      "          [-0.0148,  0.0651,  0.8700,  ...,  0.8561, -0.8034, -0.4202],\n",
      "          [ 0.2011, -0.3886,  0.6157,  ...,  0.3942,  0.4235, -0.1619]],\n",
      "\n",
      "         [[ 0.1848,  0.1869, -0.1388,  ...,  0.7511, -0.5337,  0.6015],\n",
      "          [-0.2722,  0.7725,  0.5086,  ...,  0.2859, -0.1938,  0.1051],\n",
      "          [-0.1155,  0.1739, -0.8968,  ...,  0.4809, -0.9269,  0.6018],\n",
      "          ...,\n",
      "          [-0.6942,  0.1296, -0.8125,  ..., -0.1160,  0.6992, -0.5192],\n",
      "          [-0.7908, -0.4556,  0.3742,  ..., -0.8972,  0.1325,  0.4380],\n",
      "          [-0.0088,  0.0510,  0.2996,  ..., -0.0143,  0.1451, -0.6114]],\n",
      "\n",
      "         [[-0.5258,  0.9914, -0.8065,  ...,  0.8578, -0.2347,  0.1629],\n",
      "          [-0.5298,  0.4391,  0.3366,  ...,  0.2575, -0.6902, -0.4620],\n",
      "          [-0.2915, -0.0782,  0.8575,  ...,  0.8616, -0.3835,  0.0567],\n",
      "          ...,\n",
      "          [ 0.5671, -0.3745, -0.2331,  ..., -0.3372, -0.9380,  0.5991],\n",
      "          [ 0.6870, -0.0428,  0.9049,  ..., -0.1673,  0.4565,  0.8363],\n",
      "          [ 0.3908,  0.1605, -0.8044,  ...,  0.6333, -0.6293,  0.1834]]]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir= root_dir\n",
    "        self.list_files = os.listdir(self, root_dir)\n",
    "        print(self.list_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_files)\n",
    "    \n",
    "\n",
    "    def __getitem__(self,indx):\n",
    "        img_file = self.list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def get_files(dir):\n",
    "    root_dir= dir\n",
    "    list_files = os.listdir(root_dir)\n",
    "    return list_files\n",
    "\n",
    "def get_np_file(root_dir,file_name):\n",
    "    np_path = os.path.join(root_dir,file_name)\n",
    "    np_image= np.load(np_path)\n",
    "    return np_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For training we need 2 kinds of dataset one is grayscale and another is ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ab3.npy', 'ab1.npy', 'ab2.npy'], ['gray_scale.npy'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_root_dir = '/home/selvan/Documents/ab/ab'\n",
    "l_root_dir = '/home/selvan/Documents/l'\n",
    "ab_list_files = get_files(ab_root_dir)\n",
    "l_list_files = get_files(l_root_dir)\n",
    "ab_list_files, l_list_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_sacale_img= get_np_file(l_root_dir,l_list_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 224, 224, 2), (10000, 224, 224, 2), (5000, 224, 224, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab1_npy = get_np_file(ab_root_dir,ab_list_files[1])\n",
    "ab2_npy = get_np_file(ab_root_dir,ab_list_files[2])\n",
    "ab3_npy = get_np_file(ab_root_dir,ab_list_files[0])\n",
    "ab1_npy.shape,ab2_npy.shape,ab3_npy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_train = np.concatenate((ab1_npy, ab2_npy, ab3_npy), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, data1, data2, gray_scale=True):\n",
    "        self.data1 = data1  # Grayscale data\n",
    "        self.data2 = data2  # AB data\n",
    "\n",
    "        # Define transforms based on whether the images are grayscale or color\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) if gray_scale else transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.data1), len(self.data2))  # Ensure you return the minimum length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load grayscale image\n",
    "        img1 = self.data1[idx]\n",
    "        img1 = Image.fromarray(img1.astype(np.uint8), mode='L')  # Convert to grayscale\n",
    "\n",
    "        # Load AB image\n",
    "        img2 = self.data2[idx]\n",
    "        img2 = Image.fromarray(img2.astype(np.uint8))  # Assuming AB is RGB or similar\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        \n",
    "        return img1, img2  # Return both images\n",
    "\n",
    "# Create dataset\n",
    "dataset = NumpyDataset(data1=gray_sacale_img, data2= ab_train , gray_scale=True)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([16, 1, 256, 256]) torch.Size([16, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(dataloader):\n",
    "    print(batch, X.shape,y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(gen_output, target, fake_output, lambda_L1=100):\n",
    "    gan_loss = nn.BCEWithLogitsLoss()(fake_output, torch.ones_like(fake_output))\n",
    "    l1_loss = nn.L1Loss()(gen_output, target)\n",
    "    return gan_loss + lambda_L1 * l1_loss\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = nn.BCEWithLogitsLoss()(real_output, torch.ones_like(real_output))\n",
    "    fake_loss = nn.BCEWithLogitsLoss()(fake_output, torch.zeros_like(fake_output))\n",
    "    return (real_loss + fake_loss) * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming UnetGenerator and Discriminator are defined and instantiated\n",
    "unet_generator = UnetGenerator(in_channels=1)  # Example initialization\n",
    "discriminator = Discriminator()  # Example initialization\n",
    "\n",
    "# Optimizers for the generator and discriminator\n",
    "g_optimizer = optim.Adam(unet_generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        # Train the discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        fake_color = unet_generator(X)\n",
    "\n",
    "        # Check shapes\n",
    "        print(f\"X shape: {X.shape}, fake_color shape: {fake_color.shape}, y shape: {y.shape}\")\n",
    "\n",
    "        fake_input = torch.cat([X, fake_color], dim=1)  # Concatenate inputs\n",
    "        real_input = torch.cat([X, y], dim=1)  # Make sure y has compatible channels\n",
    "\n",
    "        # Discriminator outputs\n",
    "        fake_output = discriminator(fake_input.detach(), y.detach())  # Pass y if needed\n",
    "        real_output = discriminator(real_input, y)\n",
    "\n",
    "        # Calculate discriminator loss\n",
    "        d_loss = discriminator_loss(real_output, fake_output)\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train the generator\n",
    "        g_optimizer.zero_grad()\n",
    "        fake_output = discriminator(fake_input, y)  # Pass y again\n",
    "        g_loss = generator_loss(fake_color, y, fake_output)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print losses at regular intervals\n",
    "        if batch % 1000 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{batch}/{len(dataloader)}], \"\n",
    "                  f\"D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
